## Official repository for Reproduction of CVPR2023 paper: "Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method"

## Introduction

In our reproduction project, we revisited the CVPR2023 paper entitled "Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method" which introduces a significant dataset, BAID, and a novel method, SAAN (Style-specific Art Assessment Network). This paper addresses the subjective and complex task of artistic image aesthetic assessment (AIAA) by proposing a large-scale dataset of 60,337 artistic images and a new method that integrates style-specific and generic aesthetic information. Our objective was to replicate the results presented in the paper to validate the efficacy of SAAN and explore its performance on different datasets, rewriting the code for efficiency and readability, and perform an in-depth hyperparameter sensitivity analysis.

## Methodology

### Existing Code Evaluation

- **Description**: We began by evaluating the existing code from [source/repository]. Our evaluation process involved...
- **Findings**: During the code evaluation, we noticed...

### New Data Evaluation

- **Data Collection**: We sourced additional datasets from [data source] to assess the model's robustness and generalizability. The new datasets include...
- **Results on New Data**: The model's performance on new datasets showed...

#### New Code Variant: Code Revamping

- **Refactoring Process**: The original code was revamped for efficient execution on Kaggle and local environments. We undertook extensive debugging and testing to align file paths, ensuring minimal setup hassle for future users when switching between local work-environment and Kaggle.
- **Impact of Refactoring**: 

### Hyperparameter Sensitivity Analysis

- **Approach**: We systematically varied hyperparameters such as [list a few hyperparameters] to understand their impact on model performance.
- **Key Insights**: The sensitivity analysis revealed that...

## Results

Provide a brief overview of the results, perhaps with tables or figures if appropriate.

### Reproduced Results

- **Original vs. Reproduced**: Compared to the original implementation, our reproduced results are...
- **Challenges Encountered**: We faced challenges such as...

### New Data Results

- **Consistency Across Datasets**: The model's consistency across various datasets was evaluated, revealing...

### New Code Variant Results

- **Performance Comparison**: With the refactored code, the performance [improved/remained consistent] because...

### Hyperparameter Analysis

- **Critical Hyperparameters**: Our analysis highlighted [hyperparameter] as particularly influential because...

## Discussion

Discuss any discrepancies, unexpected findings, or conclusions drawn from the reproduction effort.

